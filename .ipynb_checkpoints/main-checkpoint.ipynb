{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main文件是进行训练和验证的主体代码文件。主要内容有：  \n",
    "\n",
    "1、定义了训练集和数据集，并加载数据。 \n",
    "\n",
    "2、定义模型，以及优化器、梯度下降算法组合  \n",
    "\n",
    "3、训练模型，并可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/hjh/ProgramFiles/anaconda3/envs/pytorch_HJH/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印检查有关库的版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "0.5.0\n",
      "3.1.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义数据集：继承data.Dataset，并且定义有用的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    \n",
    "    # 对数据集进行初始化\n",
    "    def __init__(self, imgroot, csvroot=None, transform=None):\n",
    "        self.transforms = transform\n",
    "        # 有标签是训练集，不然是测试集\n",
    "        if csvroot:\n",
    "            csv = pd.read_csv(csvroot)\n",
    "            csv.info()\n",
    "            self.split = 'train'\n",
    "            self.Y = csv['Category'].tolist()\n",
    "        else:\n",
    "            self.split = 'test'\n",
    "        \n",
    "        # 这里要读取目录中的所有图片文件名，然后按数字大小排序\n",
    "        filenames = os.listdir(imgroot) \n",
    "        imgs = []\n",
    "        for k in filenames:\n",
    "            if re.match('.*\\.jpg$', k):\n",
    "                imgs.append(k)\n",
    "        imgs.sort(key=lambda k:int(k[:-4]))\n",
    "        self.imgs = [os.path.join(imgroot, k) for k in imgs]\n",
    "    \n",
    "    # 返回一个数据           \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        try:\n",
    "            pil_img = Image.open(img_path)\n",
    "        except OSError:\n",
    "            raise RuntimeError(\"couldn't read imge \" + img_path)\n",
    "        if self.transforms:\n",
    "            img = self.transforms(pil_img)\n",
    "        else:\n",
    "            img = torch.from_numpy(np.asarray(pil_img))\n",
    "        if self.split == 'train':\n",
    "            y = self.Y[index]\n",
    "            return img, y\n",
    "        else:\n",
    "            return img\n",
    "    \n",
    "    # 返回图片的数量\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义了三种变换，其中后两者用于数据增广：  \n",
    "\n",
    "transform1: 恒等变换  \n",
    "\n",
    "transform2: 对图片先放大，然后随机进行水平和竖直翻转、旋转、亮度和对比度调节等变换，最后缩小到原始尺寸。其训练结果较差。  \n",
    "\n",
    "transform3：图片只在旋转时进行截取放缩，并降低了竖直翻转和旋转缩放变换的概率。其训练结果很好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize(336),\n",
    "    \n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness = 0.5, contrast = 0.5)], p =0.5),\n",
    "    \n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "transform3 = transforms.Compose([\n",
    "    \n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.1), \n",
    "    transforms.RandomApply([transforms.RandomRotation(10, expand = False),transforms.RandomResizedCrop(224)], p =0.1),\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness = 0.5, contrast = 0.5)], p =0.5),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义了几种网络模型：  \n",
    "\n",
    "MyEasyNet：单纯地用于试水，一层卷积+relu+pooling+一层全连接层  \n",
    "\n",
    "MyVggNet：在vgg16的基础上进行修改，主要是扩展了最后一个全连接层，并修改输出向量的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEasyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyEasyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,1,5)\n",
    "        self.pool = nn.MaxPool2d(4,4)\n",
    "        self.fc1 = nn.Linear(1*55*55, 180)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 1*55*55)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class MyVggNet(nn.Module):\n",
    "    def __init__(self, num_classes = 180, feature_extract = True, device = None, path = None):\n",
    "        super(MyVggNet, self).__init__()\n",
    "        self.vgg = torchvision.models.vgg16(pretrained=True)\n",
    "        if feature_extract :\n",
    "            for param in self.vgg.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.vgg.classifier[6] = nn.Sequential(\n",
    "                          nn.Linear(4096, 1024), \n",
    "                          nn.ReLU(), \n",
    "                          nn.Dropout(0.4),\n",
    "                          nn.Linear(1024, num_classes),                   \n",
    "                          nn.LogSoftmax(dim=1))\n",
    "        if path:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "        if device:\n",
    "            self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.vgg(x)\n",
    "        return x\n",
    "    \n",
    "    def param_num(self):\n",
    "        total_params_count = sum(p.numel() for p in self.parameters())\n",
    "        print(total_params_count)\n",
    "        params_to_update_count = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(params_to_update_count)\n",
    "        return 'total_params_count = {:d}, params_to_update_count = {:d}'.format(total_params_count, params_to_update_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义了训练网络的相关函数：run,  mytrain,  myrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myvalid(model):\n",
    "    valid_loss = 0.0\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for datas in validloader:\n",
    "            imgs, labels = datas\n",
    "            imgs, labels = imgs.to(device), labels.to(device)        \n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            valid_total += labels.size(0)\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "            valid_loss += criterion(outputs, labels).item()\n",
    "\n",
    "    valid_accuracy = valid_correct/valid_total * 100.0\n",
    "    valid_loss = valid_loss / valid_total\n",
    "    return valid_accuracy, valid_loss\n",
    "\n",
    "def mytrain(model, optimizer, criterion):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for i, datas in enumerate(trainloader, 0):\n",
    "        imgs, labels = datas\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step() #update params\n",
    "\n",
    "    train_accuracy = train_correct / train_total * 100.0\n",
    "    train_loss = train_loss / train_total\n",
    "    return train_accuracy, train_loss\n",
    "\n",
    "def run(model, writer, path, optimizer, criterion, epochs = 20, best = 0.0):\n",
    "    \n",
    "    # PATH = './vgg16_adam_transforms3_epoch20.pth'\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    end = start\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_accuracy, train_loss = mytrain(model, optimizer, criterion)\n",
    "        valid_accuracy, valid_loss = myvalid(model)\n",
    "\n",
    "        writer.add_scalar('valid_accuracy', valid_accuracy, global_step=epoch)\n",
    "        writer.add_scalar('train_accuracy', train_accuracy, global_step=epoch)\n",
    "        writer.add_scalar('valid_loss', valid_loss, global_step=epoch)\n",
    "        writer.add_scalar('train_loss', train_loss, global_step=epoch)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        last = end\n",
    "        end = time.time()\n",
    "        t = end - last\n",
    "        total = end - start\n",
    "\n",
    "        print(\n",
    "            'train_acc={:.4f}, train_loss={:.4f}, valid_acc={:.4f}, valid_loss={:.4f}, epoch_t={:.2f}, total_t={:.2f}'\n",
    "            .format(train_accuracy,train_loss, valid_accuracy, valid_loss, t/60, total/60))\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print('Finished Training, best_valid_accuracy is {:.4f}'.format(best))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main的主体操作流：  \n",
    "\n",
    "1、定义了训练集和数据集，并加载数据。 \n",
    "\n",
    "其中，设置batch_size=64（经测试，128对于1块CPU来说会内存溢出）\n",
    "由于训练集的数据是按标签有序地排列，所以要设置shuffle=True，以打乱顺序，保证训练的均衡性\n",
    "\n",
    "2、定义模型，以及优化器、梯度下降算法组合  \n",
    "\n",
    "3、训练模型，并可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24497 entries, 0 to 24496\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   ID        24497 non-null  int64\n",
      " 1   Category  24497 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 382.9 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900 entries, 0 to 899\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   ID        900 non-null    int64\n",
      " 1   Category  900 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 14.2 KB\n",
      "cuda:0\n",
      "MyVggNet(\n",
      "  (vgg): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.4, inplace=False)\n",
      "        (3): Linear(in_features=1024, out_features=180, bias=True)\n",
      "        (4): LogSoftmax()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "138640372\n",
      "138640372\n",
      "total_params_count = 138640372, params_to_update_count = 138640372\n",
      "train_acc=39.9559, train_loss=0.0417, valid_acc=85.7778, valid_loss=0.0071, epoch_t=3.34, total_t=3.34\n",
      "train_acc=74.3765, train_loss=0.0154, valid_acc=94.6667, valid_loss=0.0038, epoch_t=3.41, total_t=6.74\n",
      "train_acc=81.9488, train_loss=0.0107, valid_acc=94.4444, valid_loss=0.0038, epoch_t=3.36, total_t=10.10\n",
      "train_acc=85.3778, train_loss=0.0087, valid_acc=94.2222, valid_loss=0.0032, epoch_t=3.35, total_t=13.45\n",
      "train_acc=87.3331, train_loss=0.0075, valid_acc=95.4444, valid_loss=0.0028, epoch_t=3.41, total_t=16.86\n",
      "train_acc=88.5864, train_loss=0.0067, valid_acc=95.2222, valid_loss=0.0024, epoch_t=3.35, total_t=20.22\n",
      "train_acc=90.0355, train_loss=0.0058, valid_acc=96.3333, valid_loss=0.0019, epoch_t=3.41, total_t=23.63\n",
      "train_acc=90.9091, train_loss=0.0053, valid_acc=96.6667, valid_loss=0.0023, epoch_t=3.41, total_t=27.04\n",
      "train_acc=91.1295, train_loss=0.0052, valid_acc=97.0000, valid_loss=0.0017, epoch_t=3.41, total_t=30.44\n",
      "train_acc=92.1990, train_loss=0.0046, valid_acc=96.2222, valid_loss=0.0019, epoch_t=3.35, total_t=33.80\n",
      "train_acc=92.0235, train_loss=0.0046, valid_acc=96.6667, valid_loss=0.0022, epoch_t=3.35, total_t=37.15\n",
      "train_acc=92.5419, train_loss=0.0043, valid_acc=96.7778, valid_loss=0.0021, epoch_t=3.35, total_t=40.51\n",
      "train_acc=92.7297, train_loss=0.0044, valid_acc=96.6667, valid_loss=0.0024, epoch_t=3.35, total_t=43.86\n",
      "train_acc=93.4972, train_loss=0.0039, valid_acc=97.4444, valid_loss=0.0017, epoch_t=3.41, total_t=47.26\n",
      "train_acc=93.5706, train_loss=0.0039, valid_acc=96.8889, valid_loss=0.0020, epoch_t=3.35, total_t=50.61\n",
      "train_acc=93.8319, train_loss=0.0036, valid_acc=97.4444, valid_loss=0.0014, epoch_t=3.35, total_t=53.96\n",
      "train_acc=93.9340, train_loss=0.0036, valid_acc=96.8889, valid_loss=0.0023, epoch_t=3.35, total_t=57.31\n",
      "train_acc=94.2279, train_loss=0.0035, valid_acc=97.0000, valid_loss=0.0021, epoch_t=3.35, total_t=60.67\n",
      "train_acc=93.8278, train_loss=0.0038, valid_acc=96.6667, valid_loss=0.0023, epoch_t=3.35, total_t=64.02\n",
      "train_acc=94.4442, train_loss=0.0033, valid_acc=97.4444, valid_loss=0.0012, epoch_t=3.35, total_t=67.36\n",
      "Finished Training, best_valid_accuracy is 97.4444\n"
     ]
    }
   ],
   "source": [
    "trainset = MyDataset(\n",
    "        './input/train/train',\n",
    "        './input/train.csv',\n",
    "        transform3)\n",
    "trainloader = data.DataLoader(trainset, batch_size = 64,\n",
    "                                 shuffle = True, num_workers = 4)\n",
    "trainiter = iter(trainloader)\n",
    "validset = MyDataset(\n",
    "        './input/valid/valid',\n",
    "        './input/valid.csv',\n",
    "        transform1)\n",
    "validloader = data.DataLoader(validset, batch_size = 64,\n",
    "                                 shuffle = True, num_workers = 4)\n",
    "validiter = iter(validloader)\n",
    "\n",
    "\n",
    "LOAD_PATH = None\n",
    "# LOAD_PATH =  './vgg16_adam_transforms3_epoch20.pth'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = MyVggNet(num_classes = 180, feature_extract = False, device = device, path = LOAD_PATH)\n",
    "print(model)\n",
    "print(model.param_num())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "adam_optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "sgd_optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "optimizer = adam_optimizer\n",
    "epochs = 20\n",
    "    \n",
    "SAVE_PATH = './vgg16_1.pth'\n",
    "with SummaryWriter(comment='vgg16_1')as w:\n",
    "    # w.add_graph(model)\n",
    "    run(model,  w, SAVE_PATH, optimizer, criterion, epochs = epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
